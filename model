import os
from tensorflow import keras
from keras.layers import Input, GlobalAveragePooling2D, Dense, Dropout
from keras.applications import ResNet50
from keras.optimizers import Adam
from keras.metrics import CategoricalAccuracy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ReduceLROnPlateau
import numpy as np

# Define the directories for train and validation data
train_dir = '/kaggle/working/augmented_image/train_set'
valid_dir = '/kaggle/working/augmented_image/valid_set'

# Create an ImageDataGenerator with augmentation for training
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Load augmented training images using flow_from_directory
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=64,
    class_mode='categorical',
    shuffle=True
)

# Create an ImageDataGenerator for validation data (no augmentation or rescaling)
valid_datagen = ImageDataGenerator(rescale=1./255)

# Load validation images using flow_from_directory
valid_generator = valid_datagen.flow_from_directory(
    valid_dir,
    target_size=(224, 224),
    batch_size=64,
    class_mode='categorical',
    shuffle=False
)

# Create the ResNet50 base model
base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))
base_model.trainable = True

# Fine-tune from a specific layer onwards
fine_tune_at = 100
for layer in base_model.layers[:fine_tune_at]:
    layer.trainable = False

# Add custom layers for classification
input_layer = Input(shape=(224, 224, 3))
output_base_model = base_model(input_layer, training=False)
global_pooling = GlobalAveragePooling2D()(output_base_model)
dense_one = Dense(128, activation='relu')(global_pooling)
dropout_one = Dropout(0.5)(dense_one)
dense_two = Dense(64, activation='relu')(dropout_one)
dropout_two = Dropout(0.5)(dense_two)
output_layer = Dense(4, activation='softmax')(dropout_two)

model = keras.models.Model(input_layer, output_layer)
model.compile(loss='categorical_crossentropy',
              optimizer=Adam(learning_rate=0.0001),
              metrics=[CategoricalAccuracy(name='accuracy')])

# Display the model summary
model.summary()

# Use a learning rate scheduler
lr_scheduler = ReduceLROnPlateau(factor=0.1, patience=3, min_lr=0.00001)

# Train the model
history = model.fit(train_generator, epochs=50, callbacks=[lr_scheduler])

# Predict on the entire validation dataset (which is used as our test set in this case)
predictions = model.predict(valid_generator)

# Get the predicted classes for each image batch
predicted_classes = np.argmax(predictions, axis=1)

# Map the predicted class indices to the actual class labels
class_labels = valid_generator.class_indices
class_labels = {v: k for k, v in class_labels.items()}
predicted_class_labels = [class_labels[pred] for pred in predicted_classes]

# Print the predicted class labels for each image batch
#print("Predicted Class Labels:", predicted_class_labels)

# Optionally, you can also print the confidence scores for each prediction
##confidence_scores = [predictions[i][pred] for i, pred in enumerate(predicted_classes)]
#print("Confidence Scores:", confidence_scores)
